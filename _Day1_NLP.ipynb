{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMOlu5MggE3Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')       # For tokenization\n",
        "nltk.download('stopwords')   # (Optional, for later)"
      ],
      "metadata": {
        "id": "pUc6_do-gLu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mini DataFrame with sample sentences\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love learning about machine learning and NLP!\",\n",
        "        \"Natural Language Processing is amazing ðŸ¤–ðŸ”¥\",\n",
        "        \"Data cleaning is the most important part of data science.\",\n",
        "        \"Text preprocessing involves tokenization and normalization.\",\n",
        "        \"Wow!! This course is super informative and helpful!!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4VNuu3x6g8vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic preprocessing: remove special characters and digits, then tokenize\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(cleaned)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "sFr_C_Rog9R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['tokens'] = df['text'].apply(preprocess_text)\n",
        "df[['text', 'tokens']]\n"
      ],
      "metadata": {
        "id": "kwqUfTgGiT8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5bMKBuCitQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}